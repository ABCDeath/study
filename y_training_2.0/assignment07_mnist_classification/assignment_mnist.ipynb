{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ABCDeath/study/blob/main/y_training_2.0/assignment07_mnist_classification/assignment_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uB1JcrCANcov"
   },
   "source": [
    "## Домашнее задание №7\n",
    "\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QJnmCfEDNcox"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BLQo2B5Ncoy"
   },
   "source": [
    "### Задача №1:\n",
    "Обратимся к классической задаче распознавания рукописных цифр. Мы будем работать с набором данных [MNIST](http://yann.lecun.com/exdb/mnist/). В данном задании воспользуемся всем датасетом целиком.\n",
    "\n",
    "__Ваша основная задача: реализовать весь пайплан обучения модели и добиться качества $\\geq 92\\%$ на тестовой выборке.__\n",
    "\n",
    "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбуку первого занятия.\n",
    "\n",
    "Настоятельно рекомендуем написать код \"с нуля\", лишь поглядывая на готовые примеры, а не просто \"скопировать-вставить\". Это поможет вам в дальнейшем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "PGTlQdOjNcoy",
    "outputId": "0ec16cd3-8744-4cb1-ce08-65647e85db28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image label: 8')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "# Load MNIST data using TensorFlow\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_data_loader = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_data_loader = train_data_loader.shuffle(60000).batch(32)\n",
    "\n",
    "test_data_loader = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_data_loader = test_data_loader.batch(32)\n",
    "\n",
    "# Get a random batch for visualization\n",
    "random_batch = next(iter(train_data_loader))\n",
    "_image, _label = random_batch[0][0], random_batch[1][0]\n",
    "plt.figure()\n",
    "plt.imshow(_image.numpy().reshape(28, 28))\n",
    "plt.title(f'Image label: {_label}')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wDGdpSPNcoy"
   },
   "source": [
    "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 92% accuracy.\n",
    "\n",
    "*Комментарий: для этого достаточно линейных слоев и функций активации.*\n",
    "\n",
    "__Внимание, ваша модель должна быть представлена именно переменной `model`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xokVO5QWNcoy"
   },
   "outputs": [],
   "source": [
    "shape = _image.shape[0] * _image.shape[1]\n",
    "# Creating model instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j4cryIwNcoy"
   },
   "source": [
    "Локальные тесты для проверки вашей модели доступны ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sHY4pjLvNcoz",
    "outputId": "d3a6ff65-ac60-407f-dc4c-b6bf52246138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything seems fine!\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "# assert model is not None, 'Please, use `model` variable to store your model'\n",
    "\n",
    "try:\n",
    "    x = tf.reshape(random_batch[0], (-1, 784))\n",
    "    y = random_batch[1]\n",
    "\n",
    "    # compute outputs given inputs, both are variables\n",
    "#    y_predicted = model(x)\n",
    "except Exception as e:\n",
    "    print('Something is wrong with the model')\n",
    "    raise e\n",
    "\n",
    "\n",
    "# assert y_predicted.shape[-1] == 10, 'Model should predict 10 logits/probas'\n",
    "\n",
    "print('Everything seems fine!')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0UN-ZAKNcoz"
   },
   "source": [
    "Настройте параметры модели на обучающей выборке. Рекомендуем поработать с различными оптимизаторами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layers implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(tf.Module):\n",
    "    def __init__(self, shape, activation = None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.activation = activation\n",
    "        self.w = tf.Variable(tf.random.normal(shape[::-1])*0.01, name='weights')\n",
    "        self.b = tf.Variable(tf.zeros((shape[-1],)), name='biases')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        linear_out = tf.matmul(x, tf.transpose(self.w)) + self.b\n",
    "        return self.activation(linear_out) if self.activation else linear_out\n",
    "\n",
    "class Model(tf.Module):\n",
    "    def __init__(self, *layers, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkZ-t-TcNcoz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== `Model 0` ===================\n",
      "epoch: 0, loss: 2.2948\n",
      "epoch: 1, loss: 2.2856\n",
      "epoch: 2, loss: 2.2619\n",
      "epoch: 3, loss: 1.6011\n",
      "epoch: 4, loss: 0.6450\n",
      "epoch: 5, loss: 0.3734\n",
      "epoch: 6, loss: 0.5705\n",
      "epoch: 7, loss: 0.4703\n",
      "epoch: 8, loss: 0.2512\n",
      "epoch: 9, loss: 0.3284\n",
      "epoch: 10, loss: 0.3709\n",
      "epoch: 11, loss: 0.2329\n",
      "epoch: 12, loss: 0.3263\n",
      "epoch: 13, loss: 0.3170\n",
      "epoch: 14, loss: 0.1576\n",
      "=================== `Model 1` ===================\n",
      "epoch: 0, loss: 2.3042\n",
      "epoch: 1, loss: 2.2932\n",
      "epoch: 2, loss: 2.2953\n",
      "epoch: 3, loss: 2.2665\n",
      "epoch: 4, loss: 1.6119\n",
      "epoch: 5, loss: 0.8303\n",
      "epoch: 6, loss: 0.4386\n",
      "epoch: 7, loss: 0.4740\n",
      "epoch: 8, loss: 0.3696\n",
      "epoch: 9, loss: 0.1882\n",
      "epoch: 10, loss: 0.2901\n",
      "epoch: 11, loss: 0.2430\n",
      "epoch: 12, loss: 0.2459\n",
      "epoch: 13, loss: 0.3499\n",
      "epoch: 14, loss: 0.4291\n",
      "=================== `Model 2` ===================\n",
      "epoch: 0, loss: 2.3018\n",
      "epoch: 1, loss: 2.3043\n",
      "epoch: 2, loss: 2.2957\n",
      "epoch: 3, loss: 2.1427\n",
      "epoch: 4, loss: 1.6710\n",
      "epoch: 5, loss: 0.7908\n",
      "epoch: 6, loss: 1.0549\n",
      "epoch: 7, loss: 0.4203\n",
      "epoch: 8, loss: 0.9330\n",
      "epoch: 9, loss: 0.4381\n",
      "epoch: 10, loss: 0.2735\n",
      "epoch: 11, loss: 0.3384\n",
      "epoch: 12, loss: 0.4076\n",
      "epoch: 13, loss: 0.2509\n",
      "epoch: 14, loss: 0.4364\n",
      "=================== `Model 3` ===================\n",
      "epoch: 0, loss: 2.3012\n",
      "epoch: 1, loss: 2.3048\n",
      "epoch: 2, loss: 2.3017\n",
      "epoch: 3, loss: 2.2974\n",
      "epoch: 4, loss: 2.2626\n",
      "epoch: 5, loss: 1.7515\n",
      "epoch: 6, loss: 1.3476\n",
      "epoch: 7, loss: 1.3694\n",
      "epoch: 8, loss: 1.2228\n",
      "epoch: 9, loss: 0.8539\n",
      "epoch: 10, loss: 0.4928\n",
      "epoch: 11, loss: 0.3035\n",
      "epoch: 12, loss: 0.5399\n",
      "epoch: 13, loss: 0.8215\n",
      "epoch: 14, loss: 0.1655\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(target, prediction):\n",
    "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(target, prediction))\n",
    "\n",
    "@tf.function\n",
    "def train_epoch(model, data_loader, loss_fn, lr=1e-2):\n",
    "    loss = 0.\n",
    "    for batch in data_loader:\n",
    "        images, labels = batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            x = tf.reshape(images, (-1, 784))\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(tf.cast(labels, tf.int32), logits)\n",
    "\n",
    "        trainables = tuple((layer.w,  layer.b) for layer in model.layers)\n",
    "        gradients = tape.gradient(loss, trainables)\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            dw, db = gradients[i]\n",
    "\n",
    "            layer.w.assign_sub(lr * dw)\n",
    "            layer.b.assign_sub(lr * db)\n",
    "\n",
    "    return loss\n",
    "\n",
    "models = (\n",
    "    Model(\n",
    "        DenseLayer(shape=(shape, 128), activation=tf.nn.relu),\n",
    "        DenseLayer(shape=(128, 64), activation=tf.nn.relu),\n",
    "        DenseLayer(shape=(64, 10)),\n",
    "    ),\n",
    "    Model(\n",
    "        DenseLayer(shape=(shape, 128), activation=tf.nn.relu),\n",
    "        DenseLayer(shape=(128, 32), activation=tf.nn.relu),\n",
    "        DenseLayer(shape=(32, 10)),\n",
    "    ),\n",
    "    Model(\n",
    "        DenseLayer(shape=(shape, 64), activation=tf.nn.relu),\n",
    "        DenseLayer(shape=(64, 64), activation=tf.nn.relu),\n",
    "        DenseLayer(shape=(64, 10)),\n",
    "    ),\n",
    "    Model(\n",
    "        DenseLayer(shape=(shape, 64), activation=tf.nn.relu),\n",
    "        DenseLayer(shape=(64, 16), activation=tf.nn.relu),\n",
    "        DenseLayer(shape=(16, 10)),\n",
    "    ),\n",
    ")\n",
    "\n",
    "best_model, model_loss = models[0], 1e10\n",
    "for i, model in enumerate(models):\n",
    "    print(f'=================== `Model {i}` ===================')\n",
    "\n",
    "    loss = 0.\n",
    "    for epoch in range(15):\n",
    "        loss = train_epoch(model, train_data_loader, loss_fn, lr=5e-3)\n",
    "\n",
    "        print(f'epoch: {epoch}, loss: {loss:.4f}')\n",
    "\n",
    "    if loss < model_loss:\n",
    "        best_model, model_loss = model, loss\n",
    "\n",
    "my_model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using keras entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== `Model 0` ===================\n",
      "epoch: 0, loss: 1.8813\n",
      "epoch: 1, loss: 1.3490\n",
      "epoch: 2, loss: 0.9570\n",
      "epoch: 3, loss: 0.6859\n",
      "epoch: 4, loss: 0.5017\n",
      "epoch: 5, loss: 0.6206\n",
      "epoch: 6, loss: 0.6556\n",
      "epoch: 7, loss: 0.4482\n",
      "epoch: 8, loss: 0.4511\n",
      "epoch: 9, loss: 0.3968\n",
      "epoch: 10, loss: 0.5096\n",
      "epoch: 11, loss: 0.3010\n",
      "epoch: 12, loss: 0.5576\n",
      "epoch: 13, loss: 0.3890\n",
      "epoch: 14, loss: 0.1332\n",
      "=================== `Model 1` ===================\n",
      "epoch: 0, loss: 1.9605\n",
      "epoch: 1, loss: 1.3166\n",
      "epoch: 2, loss: 1.0409\n",
      "epoch: 3, loss: 0.6691\n",
      "epoch: 4, loss: 0.6701\n",
      "epoch: 5, loss: 0.7772\n",
      "epoch: 6, loss: 0.5697\n",
      "epoch: 7, loss: 0.6321\n",
      "epoch: 8, loss: 0.5995\n",
      "epoch: 9, loss: 0.3896\n",
      "epoch: 10, loss: 0.3373\n",
      "epoch: 11, loss: 0.4824\n",
      "epoch: 12, loss: 0.3755\n",
      "epoch: 13, loss: 0.5656\n",
      "epoch: 14, loss: 0.5290\n",
      "=================== `Model 2` ===================\n",
      "epoch: 0, loss: 2.0265\n",
      "epoch: 1, loss: 1.6540\n",
      "epoch: 2, loss: 1.2779\n",
      "epoch: 3, loss: 1.0557\n",
      "epoch: 4, loss: 0.7611\n",
      "epoch: 5, loss: 0.5123\n",
      "epoch: 6, loss: 0.8692\n",
      "epoch: 7, loss: 0.5562\n",
      "epoch: 8, loss: 0.3596\n",
      "epoch: 9, loss: 0.5081\n",
      "epoch: 10, loss: 0.5266\n",
      "epoch: 11, loss: 0.2418\n",
      "epoch: 12, loss: 0.5625\n",
      "epoch: 13, loss: 0.3462\n",
      "epoch: 14, loss: 0.3912\n",
      "=================== `Model 3` ===================\n",
      "epoch: 0, loss: 1.9502\n",
      "epoch: 1, loss: 1.6065\n",
      "epoch: 2, loss: 1.2879\n",
      "epoch: 3, loss: 0.8096\n",
      "epoch: 4, loss: 0.7601\n",
      "epoch: 5, loss: 0.4964\n",
      "epoch: 6, loss: 0.4709\n",
      "epoch: 7, loss: 0.6396\n",
      "epoch: 8, loss: 0.6699\n",
      "epoch: 9, loss: 0.5157\n",
      "epoch: 10, loss: 0.3967\n",
      "epoch: 11, loss: 0.4274\n",
      "epoch: 12, loss: 0.3279\n",
      "epoch: 13, loss: 0.5630\n",
      "epoch: 14, loss: 0.5355\n",
      "=================== `Model 4` ===================\n",
      "epoch: 0, loss: 0.2315\n",
      "epoch: 1, loss: 0.0616\n",
      "epoch: 2, loss: 0.0253\n",
      "epoch: 3, loss: 0.1080\n",
      "epoch: 4, loss: 0.0228\n",
      "epoch: 5, loss: 0.0031\n",
      "epoch: 6, loss: 0.0017\n",
      "epoch: 7, loss: 0.0548\n",
      "epoch: 8, loss: 0.0060\n",
      "epoch: 9, loss: 0.0023\n",
      "epoch: 10, loss: 0.0022\n",
      "epoch: 11, loss: 0.0021\n",
      "epoch: 12, loss: 0.0320\n",
      "epoch: 13, loss: 0.0131\n",
      "epoch: 14, loss: 0.0141\n",
      "=================== `Model 5` ===================\n",
      "epoch: 0, loss: 0.2537\n",
      "epoch: 1, loss: 0.2290\n",
      "epoch: 2, loss: 0.0134\n",
      "epoch: 3, loss: 0.0578\n",
      "epoch: 4, loss: 0.0363\n",
      "epoch: 5, loss: 0.0312\n",
      "epoch: 6, loss: 0.0187\n",
      "epoch: 7, loss: 0.0150\n",
      "epoch: 8, loss: 0.0378\n",
      "epoch: 9, loss: 0.0007\n",
      "epoch: 10, loss: 0.0010\n",
      "epoch: 11, loss: 0.0071\n",
      "epoch: 12, loss: 0.0113\n",
      "epoch: 13, loss: 0.0439\n",
      "epoch: 14, loss: 0.0011\n",
      "=================== `Model 6` ===================\n",
      "epoch: 0, loss: 0.0451\n",
      "epoch: 1, loss: 0.1245\n",
      "epoch: 2, loss: 0.2333\n",
      "epoch: 3, loss: 0.0716\n",
      "epoch: 4, loss: 0.0284\n",
      "epoch: 5, loss: 0.0259\n",
      "epoch: 6, loss: 0.0030\n",
      "epoch: 7, loss: 0.1159\n",
      "epoch: 8, loss: 0.0495\n",
      "epoch: 9, loss: 0.0032\n",
      "epoch: 10, loss: 0.0129\n",
      "epoch: 11, loss: 0.0069\n",
      "epoch: 12, loss: 0.0700\n",
      "epoch: 13, loss: 0.0192\n",
      "epoch: 14, loss: 0.0144\n",
      "=================== `Model 7` ===================\n",
      "epoch: 0, loss: 0.2138\n",
      "epoch: 1, loss: 0.1756\n",
      "epoch: 2, loss: 0.3127\n",
      "epoch: 3, loss: 0.1610\n",
      "epoch: 4, loss: 0.0726\n",
      "epoch: 5, loss: 0.0660\n",
      "epoch: 6, loss: 0.0313\n",
      "epoch: 7, loss: 0.2412\n",
      "epoch: 8, loss: 0.0378\n",
      "epoch: 9, loss: 0.1995\n",
      "epoch: 10, loss: 0.0016\n",
      "epoch: 11, loss: 0.0677\n",
      "epoch: 12, loss: 0.0185\n",
      "epoch: 13, loss: 0.0067\n",
      "epoch: 14, loss: 0.0683\n"
     ]
    }
   ],
   "source": [
    "def build_model(layer1_units, layer2_units):\n",
    "    input_layer = tf.keras.layers.Input(shape=(shape,))\n",
    "    layer1 = tf.keras.layers.Dense(layer1_units, activation=tf.keras.activations.relu)(input_layer)\n",
    "    layer2 = tf.keras.layers.Dense(layer2_units, activation=tf.keras.activations.relu)(layer1)\n",
    "    output = tf.keras.layers.Dense(10)(layer2)\n",
    "\n",
    "    return tf.keras.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "@tf.function\n",
    "def train_epoch(model, data_loader, optimizer, loss_fn):\n",
    "    loss = 0.\n",
    "    for batch in data_loader:\n",
    "        images, labels = batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            x = tf.reshape(images, (-1, 784))\n",
    "            logits = model(x, training=True)\n",
    "            loss = loss_fn(tf.cast(labels, tf.int32), logits)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "lr = 5e-4\n",
    "models = (\n",
    "    (build_model(128, 64), tf.keras.optimizers.SGD(learning_rate=lr)),\n",
    "    (build_model(128, 32), tf.keras.optimizers.SGD(learning_rate=lr)),\n",
    "    (build_model(64, 64), tf.keras.optimizers.SGD(learning_rate=lr)),\n",
    "    (build_model(64, 16), tf.keras.optimizers.SGD(learning_rate=lr)),\n",
    "\n",
    "    (build_model(128, 64), tf.keras.optimizers.Adam(learning_rate=lr)),\n",
    "    (build_model(128, 32), tf.keras.optimizers.Adam(learning_rate=lr)),\n",
    "    (build_model(64, 64), tf.keras.optimizers.Adam(learning_rate=lr)),\n",
    "    (build_model(64, 16), tf.keras.optimizers.Adam(learning_rate=lr)),\n",
    ")\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "best_model, model_loss = models[0], 1e10\n",
    "for i, (model, optimizer) in enumerate(models):\n",
    "    print(f'=================== `Model {i}` ===================')\n",
    "\n",
    "    loss = 0.\n",
    "    for epoch in range(15):\n",
    "        loss = train_epoch(model, train_data_loader, optimizer, loss_fn)\n",
    "\n",
    "        print(f'epoch: {epoch}, loss: {loss:.4f}')\n",
    "\n",
    "    if loss < model_loss:\n",
    "        best_model, model_loss = model, loss\n",
    "\n",
    "keras_entities_model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== `Model 0` ===================\n",
      "Epoch 1/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 2.2041\n",
      "Epoch 2/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 1.6785\n",
      "Epoch 3/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.1883\n",
      "Epoch 4/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.8869\n",
      "Epoch 5/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.7231\n",
      "Epoch 6/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.6207\n",
      "Epoch 7/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.5568\n",
      "Epoch 8/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.5151\n",
      "Epoch 9/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.4763\n",
      "Epoch 10/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.4506\n",
      "Epoch 11/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.4285\n",
      "Epoch 12/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.4141\n",
      "Epoch 13/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.3999\n",
      "Epoch 14/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.3831\n",
      "Epoch 15/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.3757\n",
      "=================== `Model 1` ===================\n",
      "Epoch 1/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 2.2052\n",
      "Epoch 2/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 1.6974\n",
      "Epoch 3/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 1.2202\n",
      "Epoch 4/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.9023\n",
      "Epoch 5/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.7133\n",
      "Epoch 6/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.6163\n",
      "Epoch 7/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.5571\n",
      "Epoch 8/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.5073\n",
      "Epoch 9/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.4670\n",
      "Epoch 10/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.4505\n",
      "Epoch 11/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.4299\n",
      "Epoch 12/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.4131\n",
      "Epoch 13/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.3987\n",
      "Epoch 14/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.3875\n",
      "Epoch 15/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.3768\n",
      "=================== `Model 2` ===================\n",
      "Epoch 1/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.2530\n",
      "Epoch 2/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.8034\n",
      "Epoch 3/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.2983\n",
      "Epoch 4/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9381\n",
      "Epoch 5/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.7358\n",
      "Epoch 6/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.6231\n",
      "Epoch 7/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.5532\n",
      "Epoch 8/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.5129\n",
      "Epoch 9/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.4741\n",
      "Epoch 10/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.4456\n",
      "Epoch 11/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.4248\n",
      "Epoch 12/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.4104\n",
      "Epoch 13/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.3952\n",
      "Epoch 14/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.3817\n",
      "Epoch 15/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.3701\n",
      "=================== `Model 3` ===================\n",
      "Epoch 1/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 2.2698\n",
      "Epoch 2/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.0102\n",
      "Epoch 3/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.7360\n",
      "Epoch 4/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 1.4254\n",
      "Epoch 5/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 1.1388\n",
      "Epoch 6/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.9237\n",
      "Epoch 7/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.7859\n",
      "Epoch 8/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.6867\n",
      "Epoch 9/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.6193\n",
      "Epoch 10/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.5593\n",
      "Epoch 11/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.5286\n",
      "Epoch 12/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.4954\n",
      "Epoch 13/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.4703\n",
      "Epoch 14/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.4462\n",
      "Epoch 15/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.4318\n",
      "=================== `Model 4` ===================\n",
      "Epoch 1/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5371\n",
      "Epoch 2/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.1412\n",
      "Epoch 3/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0902\n",
      "Epoch 4/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0665\n",
      "Epoch 5/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0518\n",
      "Epoch 6/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0398\n",
      "Epoch 7/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0330\n",
      "Epoch 8/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0245\n",
      "Epoch 9/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0207\n",
      "Epoch 10/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0168\n",
      "Epoch 11/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0146\n",
      "Epoch 12/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0134\n",
      "Epoch 13/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0117\n",
      "Epoch 14/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0078\n",
      "Epoch 15/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0117\n",
      "=================== `Model 5` ===================\n",
      "Epoch 1/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.6046\n",
      "Epoch 2/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.1431\n",
      "Epoch 3/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0981\n",
      "Epoch 4/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0719\n",
      "Epoch 5/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0579\n",
      "Epoch 6/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0457\n",
      "Epoch 7/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0373\n",
      "Epoch 8/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0290\n",
      "Epoch 9/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0240\n",
      "Epoch 10/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0200\n",
      "Epoch 11/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0168\n",
      "Epoch 12/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0123\n",
      "Epoch 13/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0116\n",
      "Epoch 14/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0109\n",
      "Epoch 15/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0080\n",
      "=================== `Model 6` ===================\n",
      "Epoch 1/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.6188\n",
      "Epoch 2/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.1684\n",
      "Epoch 3/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.1214\n",
      "Epoch 4/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0948\n",
      "Epoch 5/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0757\n",
      "Epoch 6/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0613\n",
      "Epoch 7/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0519\n",
      "Epoch 8/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0444\n",
      "Epoch 9/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0389\n",
      "Epoch 10/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0302\n",
      "Epoch 11/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0273\n",
      "Epoch 12/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0225\n",
      "Epoch 13/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0205\n",
      "Epoch 14/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0183\n",
      "Epoch 15/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0181\n",
      "=================== `Model 7` ===================\n",
      "Epoch 1/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.7883\n",
      "Epoch 2/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.2070\n",
      "Epoch 3/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.1463\n",
      "Epoch 4/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.1251\n",
      "Epoch 5/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.1030\n",
      "Epoch 6/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0855\n",
      "Epoch 7/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0758\n",
      "Epoch 8/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0632\n",
      "Epoch 9/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0565\n",
      "Epoch 10/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0529\n",
      "Epoch 11/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0436\n",
      "Epoch 12/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0411\n",
      "Epoch 13/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0355\n",
      "Epoch 14/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0316\n",
      "Epoch 15/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0294\n"
     ]
    }
   ],
   "source": [
    "def build_model(layer1_units, layer2_units, optimizer, loss_fn):\n",
    "    model = tf.keras.models.Sequential(\n",
    "        (\n",
    "            tf.keras.layers.Input(shape=(28, 28)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(layer1_units, activation=tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dense(layer2_units, activation=tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dense(10),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "    return model\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "lr = 1e-3\n",
    "models = (\n",
    "    build_model(128, 64, tf.keras.optimizers.SGD(learning_rate=lr), loss_fn),\n",
    "    build_model(128, 32, tf.keras.optimizers.SGD(learning_rate=lr), loss_fn),\n",
    "    build_model(64, 64, tf.keras.optimizers.SGD(learning_rate=lr), loss_fn),\n",
    "    build_model(64, 16, tf.keras.optimizers.SGD(learning_rate=lr), loss_fn),\n",
    "\n",
    "    build_model(128, 64, tf.keras.optimizers.Adam(learning_rate=lr), loss_fn),\n",
    "    build_model(128, 32, tf.keras.optimizers.Adam(learning_rate=lr), loss_fn),\n",
    "    build_model(64, 64, tf.keras.optimizers.Adam(learning_rate=lr), loss_fn),\n",
    "    build_model(64, 16, tf.keras.optimizers.Adam(learning_rate=lr), loss_fn),\n",
    ")\n",
    "\n",
    "best_model, model_loss = models[0], 1e10\n",
    "for i, model in enumerate(models):\n",
    "    print(f'=================== `Model {i}` ===================')\n",
    "\n",
    "    history = model.fit(train_data_loader, epochs=15)\n",
    "\n",
    "    loss = history.history['loss'][-1]\n",
    "    if loss < model_loss:\n",
    "        best_model, model_loss = model, loss\n",
    "\n",
    "keras_model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs7dxdoTNcoz"
   },
   "source": [
    "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8WIs2WPNcoz"
   },
   "source": [
    "Оценим качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "h3XD9fOpNcoz",
    "outputId": "f4c781cd-be35-4efe-da59-14a74ddea21d"
   },
   "outputs": [],
   "source": [
    "def test_train(model, reshape=True):\n",
    "    predicted_labels = []\n",
    "    real_labels = []\n",
    "\n",
    "    for batch in train_data_loader:\n",
    "        images, labels = batch\n",
    "        x = images.numpy().reshape(-1, 784) if reshape else images\n",
    "        y_predicted = model(x)\n",
    "        predicted_labels.append(tf.argmax(y_predicted, axis=1))\n",
    "        real_labels.append(labels)\n",
    "\n",
    "    predicted_labels = tf.concat(predicted_labels, axis=0)\n",
    "    real_labels = tf.concat(real_labels, axis=0)\n",
    "    train_acc = tf.reduce_mean(tf.cast(predicted_labels == tf.cast(real_labels, tf.int64), tf.float32))\n",
    "\n",
    "    print(f'Neural network accuracy on train set: {train_acc:3.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "wL-7_MXqNcoz"
   },
   "outputs": [],
   "source": [
    "def test_test(model, reshape=True):\n",
    "    predicted_labels = []\n",
    "    real_labels = []\n",
    "\n",
    "    for batch in test_data_loader:\n",
    "        images, labels = batch\n",
    "        x = images.numpy().reshape(-1, 784) if reshape else images\n",
    "        y_predicted = model(x)\n",
    "        predicted_labels.append(tf.argmax(y_predicted, axis=1))\n",
    "        real_labels.append(labels)\n",
    "\n",
    "    predicted_labels = tf.concat(predicted_labels, axis=0)\n",
    "    real_labels = tf.concat(real_labels, axis=0)\n",
    "    test_acc = tf.reduce_mean(tf.cast(predicted_labels == tf.cast(real_labels, tf.int64), tf.float32))\n",
    "\n",
    "    print(f'Neural network accuracy on test set: {test_acc:3.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.92498\n",
      "Neural network accuracy on test set: 0.9237\n",
      "Neural network accuracy on train set: 0.99747\n",
      "Neural network accuracy on test set: 0.9776\n",
      "Neural network accuracy on train set: 0.9987\n",
      "Neural network accuracy on test set: 0.9794\n"
     ]
    }
   ],
   "source": [
    "test_train(my_model)\n",
    "test_test(my_model)\n",
    "\n",
    "test_train(keras_entities_model)\n",
    "test_test(keras_entities_model)\n",
    "\n",
    "test_train(keras_model, reshape=False)\n",
    "test_test(keras_model, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2n1dB7NNco0"
   },
   "source": [
    "Проверка, что необходимые пороги пройдены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "w9eMZtoTNco0"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Test accuracy is below 0.92 threshold",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3859270099.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.92\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test accuracy is below 0.92 threshold'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.91\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train accuracy is below 0.91 while test accuracy is fine. We recommend to check your model and data flow'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Test accuracy is below 0.92 threshold"
     ]
    }
   ],
   "source": [
    "assert test_acc >= 0.92, 'Test accuracy is below 0.92 threshold'\n",
    "assert train_acc >= 0.91, 'Train accuracy is below 0.91 while test accuracy is fine. We recommend to check your model and data flow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12S3fFwCNco0"
   },
   "source": [
    "### Сдача задания\n",
    "Загрузите файл `hw07_data_dict.npy` (ссылка есть на странице с заданием) и запустите код ниже для генерации посылки. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "lS8Ma656Nco0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-22 16:10:15--  https://raw.githubusercontent.com/girafe-ai/ml-course/23s_dd_ml/homeworks/hw07_mnist_classification/hw07_data_dict.npy\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6272438 (6.0M) [application/octet-stream]\n",
      "Saving to: ‘hw07_data_dict.npy’\n",
      "\n",
      "hw07_data_dict.npy  100%[===================>]   5.98M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2026-01-22 16:10:15 (397 MB/s) - ‘hw07_data_dict.npy’ saved [6272438/6272438]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/23s_dd_ml/homeworks/hw07_mnist_classification/hw07_data_dict.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "DFtmQLTLNco0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict_hw07.npy`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "\n",
    "assert os.path.exists('hw07_data_dict.npy'), 'Please, download `hw07_data_dict.npy` and place it in the working directory'\n",
    "\n",
    "def get_predictions(model, eval_data, step=10):\n",
    "    predicted_labels = []\n",
    "\n",
    "    for idx in range(0, len(eval_data), step):\n",
    "        batch = eval_data[idx:idx+step].reshape(-1, 784)\n",
    "        y_predicted = model(batch)\n",
    "        predicted_labels.append(tf.argmax(y_predicted, axis=1))\n",
    "\n",
    "    predicted_labels = tf.concat(predicted_labels, axis=0)\n",
    "    return predicted_labels.numpy()\n",
    "\n",
    "loaded_data_dict = np.load('hw07_data_dict.npy', allow_pickle=True)\n",
    "\n",
    "submission_dict = {\n",
    "    'train': get_predictions(keras_entities_model, loaded_data_dict.item()['train'].astype('float32')),\n",
    "    'test': get_predictions(keras_entities_model, loaded_data_dict.item()['test'].astype('float32'))\n",
    "}\n",
    "\n",
    "np.save('submission_dict_hw07.npy', submission_dict, allow_pickle=True)\n",
    "print('File saved to `submission_dict_hw07.npy`')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcde2iugNco0"
   },
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "\n",
    "submission_dict_json = {\n",
    "    'train': ','.join(str(s) for s in get_predictions(keras_entities_model, loaded_data_dict.item()['train'].astype('float32'))),\n",
    "    'test': ','.join(str(s) for s in get_predictions(keras_entities_model, loaded_data_dict.item()['test'].astype('float32'))),\n",
    "}\n",
    "\n",
    "json.dump(\n",
    "    submission_dict_json, codecs.open('submission_dict_hw07.npy', 'w', encoding='utf-8'),\n",
    "    separators=(',', ':'),\n",
    "    sort_keys=True,\n",
    "    indent=4\n",
    ") ### this saves the array in • json format\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
